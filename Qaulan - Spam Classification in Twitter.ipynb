{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T07:59:48.866329Z",
     "start_time": "2021-01-01T07:59:44.963712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in /home/hadi/anaconda3/lib/python3.7/site-packages (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:39:22.796994Z",
     "start_time": "2020-12-14T14:39:22.709477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "pd.set_option('max_columns', 1000)\n",
    "pd.set_option('max_rows', 1000)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:26:09.811313Z",
     "start_time": "2020-12-14T14:26:08.367474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @AnakJakartaID: Kalian sok menerima The Jak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @SupportPersija: Kalo emang mau usik anak t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @JakmaniaBoeloes: Tetep jaga kondusifitas b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SupportPersija: Ambil positif nya aja, Dij...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SupportPersija: Selalu utamakan kepala din...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  RT @AnakJakartaID: Kalian sok menerima The Jak...      0\n",
       "1  RT @SupportPersija: Kalo emang mau usik anak t...      0\n",
       "2  RT @JakmaniaBoeloes: Tetep jaga kondusifitas b...      0\n",
       "3  RT @SupportPersija: Ambil positif nya aja, Dij...      0\n",
       "4  RT @SupportPersija: Selalu utamakan kepala din...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_excel('tweets2.xlsx')\n",
    "print(df_data.shape[0])\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:26:10.908843Z",
     "start_time": "2020-12-14T14:26:10.860177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3020\n",
       "1    1439\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:28:30.870882Z",
     "start_time": "2020-12-14T14:28:30.862527Z"
    }
   },
   "outputs": [],
   "source": [
    " def cleaning(tweet):\n",
    "    # lowercase\n",
    "    normal_tw = tweet.lower()\n",
    "    # hapus URL\n",
    "    normal_tw = re.sub(r'((www\\.[^\\s]*)|(https?://[^\\s]*))|(pic\\.twitter\\.com/[^\\s]*)', '', normal_tw)\n",
    "    # hapus @username\n",
    "    normal_tw = re.sub(r'@[^\\s]+', '', normal_tw)\n",
    "    # hapus hashtag\n",
    "    normal_tw = re.sub(r'#[^\\s]+', '', normal_tw)\n",
    "    # hapus tanda baca\n",
    "    normal_tw = re.sub(r'[^\\w\\s]', '', normal_tw) \n",
    "    # hapus angka\n",
    "    normal_tw = re.sub(r'\\d+', '', normal_tw)\n",
    "    # remove spasi berlebih\n",
    "    normal_tw = re.sub(r'\\s+', ' ', normal_tw)\n",
    "    # trim depan belakang\n",
    "    normal_tw = normal_tw.strip()\n",
    "    # regex huruf yang berulang kaya haiiii (untuk fitur unigram)\n",
    "    normal_regex = re.compile(r\"(.)\\1{1,}\")\n",
    "    # buang huruf yang berulang\n",
    "    normal_tw = normal_regex.sub(r\"\\1\\1\", normal_tw)\n",
    "    return normal_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:28:29.383558Z",
     "start_time": "2020-12-14T14:28:29.369191Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_normalize(tweet):\n",
    "    stopwords = pd.read_csv('dataset/stopwordsID.csv', header=None)\n",
    "\n",
    "    df_kamus_singkatan = pd.read_csv('dataset/kamus_singkatan.csv')\n",
    "    df_kamus_alay = pd.read_csv('dataset/colloquial-indonesian-lexicon.csv')\n",
    "\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    token_new = []\n",
    "    for k in token:\n",
    "        if k in df_kamus_singkatan['singkatan'].values:\n",
    "            k = df_kamus_singkatan.loc[df_kamus_singkatan['singkatan']\n",
    "                                       == k, 'asli'].values[0]\n",
    "        if k in df_kamus_alay['slang'].values:\n",
    "            k = df_kamus_alay.loc[df_kamus_alay['slang']\n",
    "                                  == k, 'formal'].values[0]\n",
    "        if k not in stopwords[0].values:\n",
    "            token_new.append(k)\n",
    "\n",
    "    str_clean = ' '.join(token_new)\n",
    "    return str_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:28:28.090686Z",
     "start_time": "2020-12-14T14:28:28.077094Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemming(tweet):\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    stem_kalimat = []\n",
    "    for k in token:\n",
    "        stem_kata = stemmer.stem(k)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "\n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)\n",
    "    return stem_kalimat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:28:26.815515Z",
     "start_time": "2020-12-14T14:28:26.801777Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(list_tweet):\n",
    "    tweet_clean = []\n",
    "    for tw in list_tweet:\n",
    "        normal_tweet = cleaning(tw)\n",
    "        normal_tweet = remove_stopwords_and_normalize(normal_tweet)\n",
    "#         normal_tweet = stemming(normal_tweet)\n",
    "        tweet_clean.append(normal_tweet)\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:28:25.338371Z",
     "start_time": "2020-12-14T14:28:25.323517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4459"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweet = df_data['text']\n",
    "len(raw_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:33:05.119062Z",
     "start_time": "2020-12-14T14:28:41.828487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sok menerima the jak sediakan kuota hadir bantul biar tribun noda',\n",
       " 'usik anak the jak pakai atribut asli provokasi sok berani intinya',\n",
       " 'jaga kondusifitas bantul hargai tuan rumah tahan menang']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet = preprocessing(raw_tweet)\n",
    "clean_tweet[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:33:05.124482Z",
     "start_time": "2020-12-14T14:33:05.121215Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_data['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:33:05.499447Z",
     "start_time": "2020-12-14T14:33:05.127243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['__', '_selamat', 'aa']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function to extract TF (1-gram) features\n",
    "'''\n",
    "def tf_extraction(text, ngram_start, ngram_end):\n",
    "    ngram = CountVectorizer(ngram_range=(ngram_start, ngram_end))\n",
    "    ngram_matrix = ngram.fit_transform(np.array(text)).todense()\n",
    "    feature_names = ngram.get_feature_names()\n",
    "    return ngram_matrix, feature_names\n",
    "\n",
    "# unigram features\n",
    "ngram_feat, feature_names = tf_extraction(clean_tweet, 1, 1)\n",
    "print(ngram_feat[:3])\n",
    "print(feature_names[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:33:06.925104Z",
     "start_time": "2020-12-14T14:33:05.501358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 0, 28, 140, 0], [8, 3, 27, 140, 0], [25, 0, 26, 138, 0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Function to extract orthography and url occurence features\n",
    "'''\n",
    "def orthography_and_url_extraction(text):\n",
    "    all_orto_feat = []\n",
    "    for t in text:\n",
    "        capital_count = sum(1 for c in t if c.isupper())\n",
    "        exclamation_count = sum(1 for c in t if c == \"!\")\n",
    "        word_len = len(nltk.word_tokenize(t))\n",
    "        char_len = len(t)\n",
    "        url = 1 if 'http' in t.lower() or 'www' in t.lower() else 0\n",
    "        orto_feat = [capital_count, exclamation_count, word_len, char_len, url]\n",
    "        all_orto_feat.append(orto_feat)\n",
    "    return all_orto_feat\n",
    "\n",
    "orto_feat = orthography_and_url_extraction(raw_tweet)\n",
    "orto_feat[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:33:07.267132Z",
     "start_time": "2020-12-14T14:33:06.927363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function to extract TF-IDF (1-gram) features\n",
    "'''\n",
    "def tf_idf_extraction(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(np.array(text)).todense()\n",
    "    return tfidf_matrix\n",
    "\n",
    "# tf-idf features\n",
    "tfidf_feat = tf_idf_extraction(clean_tweet)\n",
    "print(tfidf_feat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T14:15:16.902773Z",
     "start_time": "2020-12-12T12:15:08.128316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features :  tf\n",
      "Classifier :  Multilayer Perceptron\n",
      "Accuracy: 0.9271159369174182\n",
      "F1-Score: 0.9176366201263171\n",
      "Precision: 0.9153103193501154\n",
      "Recall: 0.924557598398658\n",
      "==========================================================================================\n",
      "Features :  tf-idf\n",
      "Classifier :  Multilayer Perceptron\n",
      "Accuracy: 0.9354139164609261\n",
      "F1-Score: 0.9269566504615068\n",
      "Precision: 0.9249255913368561\n",
      "Recall: 0.933236369039349\n",
      "==========================================================================================\n",
      "Features :  tf and orthography\n",
      "Classifier :  Multilayer Perceptron\n",
      "Accuracy: 0.9398972136846879\n",
      "F1-Score: 0.9325139832514504\n",
      "Precision: 0.9349322088206315\n",
      "Recall: 0.9369109459341247\n",
      "==========================================================================================\n",
      "Features :  tf-idf and orthography\n",
      "Classifier :  Multilayer Perceptron\n",
      "Accuracy: 0.9439366151055577\n",
      "F1-Score: 0.9364854117490078\n",
      "Precision: 0.9392624558039737\n",
      "Recall: 0.9404360563094005\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# list of features combinations\n",
    "feat_list = [ngram_feat, tfidf_feat, np.hstack((ngram_feat, orto_feat)), np.hstack((tfidf_feat, orto_feat))]\n",
    "feat_name = ['tf', 'tf-idf', 'tf and orthography', 'tf-idf and orthography']\n",
    "\n",
    "# list of model to do prediction\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "model_list = [mlp]\n",
    "model_name = ['Multilayer Perceptron']\n",
    "\n",
    "# build the model and evaluate the performance of it for each feature combination\n",
    "df_recap = pd.DataFrame()\n",
    "for f, fn in zip(feat_list, feat_name):\n",
    "    print(\"Features : \", fn)\n",
    "    X = f\n",
    "    y = label\n",
    "    for m, n in zip(model_list, model_name):\n",
    "        scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "        scores = cross_validate(m, X, y, cv=10, scoring=scoring)\n",
    "        acc = np.mean(scores['test_accuracy'])\n",
    "        f1 = np.mean(scores['test_f1_macro'])\n",
    "        precision = np.mean(scores['test_precision_macro'])\n",
    "        recall = np.mean(scores['test_recall_macro'])\n",
    "        print(\"Classifier : \", n)\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"F1-Score:\", f1)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        df_recap = df_recap.append({\n",
    "            'features': fn,\n",
    "            'classifier': n,\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }, ignore_index=True)\n",
    "        print('='*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T16:35:25.122556Z",
     "start_time": "2020-12-12T16:35:23.220716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.927116</td>\n",
       "      <td>0.915310</td>\n",
       "      <td>0.924558</td>\n",
       "      <td>0.917637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tf-idf</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>0.924926</td>\n",
       "      <td>0.933236</td>\n",
       "      <td>0.926957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tf and orthography</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.939897</td>\n",
       "      <td>0.934932</td>\n",
       "      <td>0.936911</td>\n",
       "      <td>0.932514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tf-idf and orthography</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.943937</td>\n",
       "      <td>0.939262</td>\n",
       "      <td>0.940436</td>\n",
       "      <td>0.936485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features             classifier  accuracy  precision  \\\n",
       "0                      tf  Multilayer Perceptron  0.927116   0.915310   \n",
       "1                  tf-idf  Multilayer Perceptron  0.935414   0.924926   \n",
       "2      tf and orthography  Multilayer Perceptron  0.939897   0.934932   \n",
       "3  tf-idf and orthography  Multilayer Perceptron  0.943937   0.939262   \n",
       "\n",
       "     recall  f1_score  \n",
       "0  0.924558  0.917637  \n",
       "1  0.933236  0.926957  \n",
       "2  0.936911  0.932514  \n",
       "3  0.940436  0.936485  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recap[['features', 'classifier', 'accuracy', 'precision', 'recall', 'f1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T16:35:09.412832Z",
     "start_time": "2020-12-12T16:35:07.496861Z"
    }
   },
   "outputs": [],
   "source": [
    "df_recap[['features', 'classifier', 'accuracy', 'precision', 'recall', 'f1_score']].to_csv('df_recap.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:39:22.703304Z",
     "start_time": "2020-12-14T14:35:52.600972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(random_state=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = np.hstack((tfidf_feat, orto_feat))\n",
    "y = label\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "mlp.fit(feat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:40:20.359012Z",
     "start_time": "2020-12-14T14:40:20.130325Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(mlp, open('final_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:17.325894Z",
     "start_time": "2020-12-14T14:57:17.060130Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(np.hstack((tfidf_feat, orto_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:41.100607Z",
     "start_time": "2020-12-14T14:57:41.087193Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:51.127612Z",
     "start_time": "2020-12-14T14:57:51.085067Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.to_csv('df_data_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:43.809864Z",
     "start_time": "2020-12-14T14:58:43.798623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Nah begini dong 4-4-2 ✊ #PERSIJADAY 🐅🔴⚪ https:...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>Emang gitu #DM4Jabar #demiz #DeddyDediAsyik #p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>Belum lah. Kan masih naik\\n\\n#AsianGamesKita \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>https://t.co/r1BXfwfbKj #PersibDay</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  prediction\n",
       "1009  Nah begini dong 4-4-2 ✊ #PERSIJADAY 🐅🔴⚪ https:...      0           1\n",
       "1402  Emang gitu #DM4Jabar #demiz #DeddyDediAsyik #p...      0           1\n",
       "1977  Belum lah. Kan masih naik\\n\\n#AsianGamesKita \\...      0           1\n",
       "4298                 https://t.co/r1BXfwfbKj #PersibDay      1           0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[df_data.label != df_data.prediction]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
